# Лабораторная работа по курсу "Искусственный интеллект"
# Многослойный персептрон

| Студент | Моисеенков И. П. |
|------|------|
| Группа  | М8О-308Б-19 |
| Оценка 1 (свой фреймворк) | *X* |
| Оценка 2 (PyTorch/Tensorflow) | *X* |
| Проверил | Сошников Д.В. |

> *Комментарии проверяющего*
### Задание

Решить задачу классификации для датасетов MNIST, FashionMNIST, CIFAR-10 с помощью 1, 2 и 3-слойного персептрона. Попробовать разные передаточные функции, провести сравнительную оценку решений. Решение сделать двумя способами:
* "С нуля", на основе базовых операций библиотеки numpy. Решение желательно реализовать в виде библиотеки, пригодной для решения более широкго круга задач.
* На основе одного из существующих нейросетевых фреймворков, в соответствии с вариантом задания:
   1. PyTorch
   1. Tensorflow/Keras

> *Номер варианта вычисляется по формуле 1 + (N-1) mod 2, где N - номер студента в списке.*

Решение оформить в файлах [Solution_MyFramework.ipynb](Solution_MyFramework.ipynb) и [Solution.ipynb](Solution.ipynb). 
Отчет по работе и сравнение методов пишется в этом файле после задания.
### Критерии оценки

Первая часть лабораторной работы:

| Сделано | Баллы |
|---------|-------|
| Реализован однослойный персептрон, классифицирующий датасет с точностью >85% | 1 |
| Реализован многослойный персептрон, классифицирующий датасет с точностью >85% | 2 |
| Реализация сделана как библиотека для обучения сетей различных конфигураций, в соответствии с примером | 1 |
| Улучшена архитектура библиотеки, отдельно вынесены алгоритмы обучения, функции потерь | 3 |
| Проведено сравнение различных гиперпараметров, таких, как передаточные функции, число нейронов в промежуточных слоях, функции потерь, с графиками обучения и матрицами неточности | 2 |
| Проведен анализ для датасета FashionMNIST | 1 |

Вторая часть лабораторной работы:

| Сделано | Баллы |
|---------|-------|
| Реализован однослойный персептрон, классифицирующий датасет с точностью >85% | 1 |
| Реализован многослойный персептрон, классифицирующий датасет с точностью >85% | 2 |
| Реализация использует возможности базового фреймворка, включая работу с данными | 3 |
| Проведено сравнение различных гиперпараметров, таких, как передаточные функции, число нейронов в промежуточных слоях, функции потерь, с графиками обучения и матрицами неточности | 2 |
| Проведен анализ для датасета FashionMNIST | 1 |
| Проведен анализ для другого датасета с цветными картинками (CIFAR-10) | 1 |

## Отчёт по работе

### Часть 1

В первой части этой работы я реализовал собственный фреймворк для обучения нейросетей. Я разбил слои и функции для обучения на независимые классы, из которых можно конструировать модель.

Мой фреймворк включает в себя реализацию следующих классов:

Слои: Linear  
Функции активации: Tanh, ReLU, Softmax  
Оптимайзеры: SGD  
Функции потерь: Cross Entropy Loss

Все классы включают в себя методы для прямого прохода по сети (forward) и обратного (backward).

С помощью своей библиотеки я решил задачи классификации для двух датасетов - мнист и фешен мнист.

#### Mnist 1 layer
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/my_fw/1l_loss.png" width="512"/>
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/my_fw/1l_acc.png" width="512"/>

```
Confusion matrix:
[[787   0   4   1   1  10  15   1   0   2]
 [  0 888   3   7   0   3   3   1  12   1]
 [ 20   4 723  16  23   4  12  29  31   9]
 [  1   2  24 755   0  47   9   8  32  16]
 [  1   2   7   2 736   0   6   4  15  39]
 [ 16   4  10  28  10 609  19   2  36  10]
 [  7   1   6   2   7  16 786   1   5   1]
 [  6   5  17   4  16   1   1 744   5  36]
 [  5  16   6  24   5  31   7   4 688  10]
 [ 15   1   4  13  26  10   0  25  22 761]]
 
```

Параметры обучения:
* 1 линейный слой
* 50 эпох
* batch size = 16
* learning rate = 1e-5
* loss - cross entropy

**Accuracy = 0.89**

#### Mnist 2 layers (tanh)
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/my_fw/2l_loss_tanh.png" width="512"/>
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/my_fw/2l_acc_tanh.png" width="512"/>

```
Confusion matrix:
[[757   0   8  10   1  15  15  10   4   1]
 [  0 890   3   4   0   1   4   3  12   1]
 [ 17  13 692  36  21   6  24  28  29   5]
 [  9   4  48 721   6  41  12  19  22  12]
 [  2   3  10   4 677   4  18   6  12  76]
 [ 23   9   9  51  16 551  27   4  39  15]
 [ 16   3  15   2  13  12 755   1  14   1]
 [  5  15  13   7  13   2   2 724  10  44]
 [  7  16  11  46   4  36  12  10 638  16]
 [  8   5  21  21  60   4   2  63  16 677]]
 
```

Параметры обучения:
* 2 линейных слоя. Промежуточный имеет размерность 128
* 100 эпох
* batch size = 16
* learning rate = 1e-5
* loss - cross entropy

**Accuracy = 0.843**

#### Mnist 2 layers (relu)
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/my_fw/2l_loss_relu.png" width="512"/>
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/my_fw/2l_acc_relu.png" width="512"/>

```
Confusion matrix:
[[789   0   5   2   1   6  14   2   1   1]
 [  0 890   7   4   2   1   1   4   8   1]
 [  8   9 760  14  13   9   9  29  15   5]
 [  2   2  24 789   0  25  10  12  23   7]
 [  1   0   1   0 753   0   9   7   5  36]
 [ 14   2   6  24  12 641  15   2  19   9]
 [ 10   6   1   0   8  12 790   0   3   2]
 [  2   8  15   4  17   3   1 758   5  22]
 [  7  15   7  17   1  12  10   5 714   8]
 [  9   1   4  20  49   7   2  29  13 743]]
 
```

Параметры обучения:
* 2 линейных слоя. Промежуточный имеет размерность 64
* 50 эпох
* batch size = 16
* learning rate = 1e-5
* loss - cross entropy

**Accuracy = 0.908**


Даже двуслойная линейная нейросеть научилась различать цифры мниста с точностью 90%. Это очень круто! Я уверен, что если добавить больше слоев и больше эпох, то мы сможем получить более высокие метрики.

Однако с фешен мнистом оказалось немного сложнее. Распознавать одежду - более трудная задача для перцептрона.

#### Fashion mnist 1 layer
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/my_fw/1l_loss_fash.png" width="512"/>
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/my_fw/1l_acc_fash.png" width="512"/>

```
Confusion matrix:
[[960   0  33   0   0   3   0   4   0   0]
 [928   0  65   0   0   1   0   0   0   6]
 [ 42   0 955   0   0   2   0   1   0   0]
 [855   0 124   0   0   0   0   1   0  20]
 [ 76   0 923   0   0   1   0   0   0   0]
 [  4   0   2   0   0 871   0  53   0  70]
 [383   0 615   0   0   1   0   0   0   1]
 [  0   0   0   0   0  31   0 862   0 107]
 [ 91   0 466   0   0 399   0  37   0   7]
 [  0   0   0   0   0  10   0  18   0 972]]
 
```

Параметры обучения:
* 1 линейный слой
* 100 эпох
* batch size = 16
* learning rate = 3e-5
* loss - cross entropy

**Accuracy = 0.462**

#### Fashion mnist 2 layers
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/my_fw/2l_loss_fash.png" width="512"/>
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/my_fw/2l_acc_fash.png" width="512"/>

```
Confusion matrix:
[[729  19  29  70   8   1 122   1  20   1]
 [  6 961   5  21   2   1   4   0   0   0]
 [ 21   9 715  15 155   1  76   0   8   0]
 [ 27  25  16 880  29   0  20   0   3   0]
 [  2   5  84  60 740   0 101   0   8   0]
 [  1   0   2   1   2 936   0   0  17  41]
 [145  11  95  48  98   0 583   0  20   0]
 [  0   0   0   0   0 649   0   0  41 310]
 [  5   4   6   6   4   7  13   0 953   2]
 [  0   0   0   0   0  14   0   0   3 983]]
 
```

Параметры обучения:
* 2 линейных слоя. Промежуточный имеет размерность 64. Разделены функцией ReLU
* 50 эпох
* batch size = 16
* learning rate = 1e-5
* loss - cross entropy

**Accuracy = 0.748**

#### Fashion mnist 3 layers
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/my_fw/3l_loss_fash.png" width="512"/>
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/my_fw/3l_acc_fash.png" width="512"/>

```
Confusion matrix:
[[759   6  24  70   2   5 116   1  16   1]
 [  2 962   7  22   0   2   4   0   1   0]
 [ 17   1 758  17 128   1  66   0  12   0]
 [ 39  21  12 864  37   0  19   0   8   0]
 [  5   4 100  37 781   0  67   0   6   0]
 [  2   1   0   4   1 876   0  55   9  52]
 [140   3 105  51  93   1 584   0  23   0]
 [  0   0   0   0   0  35   0 879   3  83]
 [  6   2  11   9   7   5  13   6 939   2]
 [  0   0   0   0   0  24   0  33   4 939]]
 
```

Параметры обучения:
* 3 линейных слоя. Промежуточные имеют размерность 128 и 32. Разделены функцией ReLU
* 100 эпох
* batch size = 16
* learning rate = 1e-5
* loss - cross entropy

**Accuracy = 0.834**

Для получения более высокой точности пришлось использовать трехслойный перцептрон. Чтобы получить еще более высокий эккюраси можно использовать сверточные слои для извлечения признаков с картинки.

### Часть 2

Во второй части работы я должен был решить задачи классификации с помощью нейросетей, используя фреймворк **PyTorch**. С его помощью гораздо удобнее работать с нейронными сетями, а также можно использовать более продвинутые алгоритмы.

#### Mnist 1 layer
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/1l_loss.png" width="512"/>
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/1l_acc.png" width="512"/>

```
Confusion matrix:
[[ 959    0    2    1    0    4   10    2    1    0]
 [   0 1111    2    2    0    1    4    2   11    0]
 [   7   11  937   11   11    3   13    8   23    6]
 [   5    2   25  900    0   35    4   11   16   10]
 [   1    3    6    1  917    0   10    3    7   32]
 [  10    3    4   26   10  780   21    5   25    6]
 [  10    3    5    1   10    9  914    2    2    0]
 [   3   14   23    5   10    1    0  937    2   32]
 [   8   14    7   21   10   31   12   10  852    8]
 [   9    8    1    8   36    6    1   25   10  904]]
 
```

Параметры обучения:
* 1 линейный слой
* 10 эпох
* batch size = 128
* optimizer = Adam, lr = 1e-3
* loss - cross entropy

**Accuracy = 0.923**

#### Mnist 2 layers
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/2l_loss.png" width="512"/>
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/2l_acc.png" width="512"/>

```
Confusion matrix:
[[ 967    0    0    0    0    5    1    2    2    2]
 [   0 1123    2    2    0    1    1    1    3    0]
 [   8    2  981    8    4    1    5   10   11    0]
 [   0    0    5  978    1   11    0    7    5    1]
 [   0    0    2    1  943    2    4    5    1   22]
 [   5    1    1   15    3  848    6    2    7    2]
 [   7    3    1    0    8    6  926    2    3    0]
 [   0    4    6    8    0    0    0  997    0   12]
 [   7    1    3   10    5    7    6    6  922    6]
 [   6    3    1    4    9    3    0    8    4  970]]
 
```

Параметры обучения:
* 2 линейных слоя. Промежуточный имеет размерность 64. Функция активации - релу
* 10 эпох
* batch size = 128
* optimizer = Adam, lr = 1e-3
* loss - cross entropy

**Accuracy = 0.967**

С помощью торча мы смогли получить более качественную модель для классификации мниста.

#### Fashion mnist 1 layer
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/1l_loss_fash.png" width="512"/>
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/1l_acc_fash.png" width="512"/>

```
Confusion matrix:
[[845   0  10  37   6   4  90   0   8   0]
 [  4 953   2  29   6   0   2   0   1   0]
 [ 32   3 685  12 154   1 102   0  10   0]
 [ 46   8  12 854  27   0  48   0   4   0]
 [  0   1  80  37 782   1  90   0   8   0]
 [  1   0   0   1   0 919   0  57   4  15]
 [155   1 107  31 112   3 566   0  24   0]
 [  0   0   0   0   0  31   0 949   0  19]
 [  7   1   8  13   2  10  21   7 928   0]
 [  0   0   0   0   0  22   0  53   1 922]]
 
```

Параметры обучения:
* 1 линейный слой
* 10 эпох
* batch size = 128
* optimizer = Adam, lr = 1e-3
* loss - cross entropy

**Accuracy = 0.842**

#### Fashion mnist 2 layers
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/2l_loss_fash.png" width="512"/>
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/2l_acc_fash.png" width="512"/>

```
Confusion matrix:
[[756   2  17  79   2   1 136   0   7   0]
 [  0 966   0  25   3   0   2   0   1   0]
 [ 10   0 836  24  67   1  60   0   1   0]
 [ 13  11   8 935  10   0  19   0   3   0]
 [  0   1 121  57 775   0  45   0   0   0]
 [  0   0   0   1   0 946   0  31   1  18]
 [ 91   0 112  61  85   0 640   0  10   0]
 [  0   0   0   0   0  27   0 959   0  13]
 [  6   1  10  10   4   5  11   4 946   0]
 [  1   0   0   0   0   8   0  55   0 934]]
 
```

Параметры обучения:
* 2 линейных слоя. Промежуточный имеет размерность 64. Функция активации - релу
* 10 эпох
* batch size = 128
* optimizer = Adam, lr = 1e-3
* loss - cross entropy

**Accuracy = 0.871**

#### Fashion mnist 3 layers
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/3l_locc_fash.png" width="512"/>
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/3l_acc_fash.png" width="512"/>

```
Confusion matrix:
[[839   2  13  22   3   0 111   1   9   0]
 [  2 971   1  15   4   0   2   0   2   0]
 [ 15   0 781  15 113   0  73   0   2   0]
 [ 28  13  10 880  28   0  32   0   8   0]
 [  1   1  65  32 840   0  58   0   2   0]
 [  0   0   0   0   0 843   0 118   2  34]
 [123   2  79  21  77   0 682   0  15   0]
 [  0   0   0   0   0   3   0 981   0  15]
 [  6   0   0   4   3   1   6   7 970   0]
 [  0   0   0   0   0   4   1  59   0 934]]
 
```

Параметры обучения:
* 3 линейных слоя. Промежуточные имеет размерность 128, 32. Функция активации - релу
* 10 эпох
* batch size = 128
* optimizer = Adam, lr = 1e-3
* loss - cross entropy

**Accuracy = 0.873**

С черно-белой одеждой нейронки из фреймворка тоже справились лучше. Попробуем проверить перцептрон на цветных цифрах из датасета CIFAR10.


#### Cifar 1 layer
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/1l_loss_cifar.png" width="512"/>
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/1l_acc_cifar.png" width="512"/>

```
Confusion matrix:
[[467  38  42  33  29  72  20  48 181  68]
 [ 67 450  16  73  24  27  29  44  89 180]
 [104  35 251 145 107  81 120  64  60  32]
 [ 42  49  75 321  59 173  95  44  59  80]
 [ 59  23 133 142 280  71 127  94  39  32]
 [ 47  40 105 232  77 262  63  66  74  31]
 [ 17  37  68 246  94  62 376  33  29  38]
 [ 65  37  49 104  95  81  30 417  47  72]
 [169  53  16  42  12  38   9  15 526 117]
 [ 91 153  16  45  14  24  31  44 112 470]]
 
```

Параметры обучения:
* 1 линейный слой
* 10 эпох
* batch size = 128
* optimizer = Adam, lr = 1e-3
* loss - cross entropy

**Accuracy = 0.383**

#### Cifar 2 layers
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/2l_loss_cifar.png" width="512"/>
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/2l_acc_cifar.png" width="512"/>

```
Confusion matrix:
[[534  39  73  11  31  25  27  37 165  56]
 [ 31 555  24  20  13  21  35  33  94 173]
 [ 65  11 379  65 117 113 110  85  26  28]
 [ 22  21  65 273  47 270 136  71  39  53]
 [ 41   7 116  56 361  72 163 132  35  17]
 [ 21   7  75 139  56 458  87  91  27  36]
 [  8  24  62  68  62  61 641  25  26  23]
 [ 38  11  42  37  62 100  42 592  20  53]
 [ 82  51  16  17  23  28   9  16 688  67]
 [ 35 170  21  21  15  31  23  40  88 556]]
 
```

Параметры обучения:
* 2 линейных слоя. Промежуточный имеет размерность 64. Функция активации - релу
* 10 эпох
* batch size = 128
* optimizer = Adam, lr = 1e-3
* loss - cross entropy

**Accuracy = 0.505**

#### Cifar 3 layers
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/3l_loss_cifar.png" width="512"/>
<img src="https://github.com/MAILabs-AI-2022/lab_perceptron-mosikk/blob/main/img/torch/3l_acc_cifar.png" width="512"/>

```
Confusion matrix:
[[584  27  57  19  25  14  25  25 157  65]
 [ 32 620  15  26   5  10  13  28  93 157]
 [ 84  12 426  66  90 104 100  77  27  13]
 [ 37  20 101 278  43 235 130  66  37  50]
 [ 55  14 167  52 366  60 124 116  28  18]
 [ 19  18  80 158  40 483  70  69  30  30]
 [  6  19  81  64  68  83 621  23  18  17]
 [ 42  11  59  46  40  89  34 604  21  51]
 [ 82  65  17  15  14  18   8   4 711  63]
 [ 39 155  21  28   5  21  25  32  76 598]]
 
```

Параметры обучения:
* 3 линейных слоя. Промежуточные имеет размерность 128, 32. Функция активации - релу
* 10 эпох
* batch size = 128
* optimizer = Adam, lr = 1e-3
* loss - cross entropy

**Accuracy = 0.53**

С цветными картинками перцептрон справился плохо. Дело в том, что перцептрон в принципе сам по себе плохо подходит для работы с изображениями в явном виде. Он никак не учитывает пространственную информацию картинки и плохо работает с цветными изображениями из-за трех каналов. А также получается довольно большое количество параметров (784 * 10 + 10 для однослойного перцептрона и мниста). Поэтому картинки лучше сначала прогонять через сверточную сеть, чтобы получить признаки для полносвязной сети. Таких признаков будет гораздо меньше, и при этом они будут нести в себе гораздо большую информацию.

Но с черно-белыми простыми картинками хорошо справились даже линейные модели. При этом при использовании фреймворка мы получили более высокий результат. Это произошло по следующим причинам:
1. Мы использовали более сложный алгоритм обучения - Адам. Он автоматически подбирает наиболее выгодный lr на каждом шаге, а также накапливает "импульс" с прошлых шагов. Тем самым он быстрее и точнее сходится к глобальному минимуму функции.
2. Мы выполнили нормировку данных, тем самым немного упростили жизнь нейросети.


## Вывод
В данной работе я написал собственную библиотеку для обучения нейронных сетей. Конечно, ее еще можно дорабатывать и дорабатывать - можно дописать сверточные и рекуррентные слои, дополнительные функции активации, методы регуляризации, функции потерь для регрессии итд. Но даже с небольшим подмножеством "объектов" нейросетей, которые я описал, я смог получить хорошую библиотеку, которая легко расширяется. Я успешно протестировал свою библиотеку и с ее помощью написал классификаторы черно-белых цифр и деталей одежды.

Во второй части работы я познакомился с одним из самых популярных фреймворков для обучения нейронных сетей - PyTorch. Он более гибкий по сравнению с TensorFlow/Keras. При этом в нем уже реализованы все необходимые компоненты для работы с нейросетями, данными, картинками, текстами. Я использовал более продвинутые техники и поэтому получил более высокую точность на классификации цифр и одежды.

В целом нейронные сети - это очень мощный механизм. Они продолжают нас удивлять каждый день. Так что понимание нейросетей сегодня - это очень полезный и востребованный навык.

## Codespaces

Я начинал делать работу в гитхаб кодспейсес.

С одной стороны, это очень удобно - открывается типа вс код в браузере с приятным интерфейсом, подсветкой синтаксиса и подсказками.

Но когда мой код начал разрастаться, кодспейсес начал слишком часто падать (даже когда я вообще не запускал код), поэтому я решил перейти на локалку и колаб.

## Материалы для изучения

 * [Реализация своего нейросетевого фреймворка](https://github.com/shwars/NeuroWorkshop/blob/master/Notebooks/IntroMyFw.ipynb)
 * [Введение в PyTorch](https://github.com/shwars/NeuroWorkshop/blob/master/Notebooks/IntroPyTorch.ipynb)
 * [Введение в Tensorflow/Keras](https://github.com/shwars/NeuroWorkshop/blob/master/Notebooks/IntroKerasTF.ipynb)
